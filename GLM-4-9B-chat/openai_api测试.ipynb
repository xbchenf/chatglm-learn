{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6637682c-5b77-4701-a40e-7d91f4b3ab1b",
   "metadata": {},
   "source": [
    "# 定义模型客户端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b1765be-15f1-4f1c-987f-37691dcba368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AJ56zgVTbBQb47Seewbva8Ye3mmP6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于2024共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复和支持。', role='assistant', function_call=None, tool_calls=None))], created=1722405795, model='glm-4', object='chat.completion', service_tier=None, system_fingerprint='fp_hawbL1lgv', usage=CompletionUsage(completion_tokens=45, prompt_tokens=25, total_tokens=70))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "base_url = \"http://127.0.0.1:8000/v1/\"\n",
    "client = OpenAI(api_key=\"EMPTY\", base_url=base_url)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\",\"content\": \"你是一个AI智能助手！\"},\n",
    "    { \"role\": \"user\",\"content\": \"你是谁\"}\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"glm-4\",\n",
    "    messages=messages,\n",
    "    stream=False,\n",
    "    max_tokens=256,\n",
    "    temperature=0.4,\n",
    "    presence_penalty=1.2,\n",
    "    top_p=0.8,\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace97933-0f1c-418f-b8e6-336b2fc2c741",
   "metadata": {},
   "source": [
    "# 定义工具函数信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b75f055-a164-4f95-8215-59f58f4d95c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-RYKvJIrYy5if1weSPDt07byh9Nzij', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_oS13YglBeAyqBUwutEVmyBNP', function=Function(arguments='{\"location\": \"San Francisco, CA\", \"format\": \"celsius\"}', name='get_current_weather'), type='function', index=0)]))], created=1722395612, model='glm-4', object='chat.completion', service_tier=None, system_fingerprint='fp_1Fk34RSST', usage=CompletionUsage(completion_tokens=21, prompt_tokens=222, total_tokens=243))\n"
     ]
    }
   ],
   "source": [
    "def function_chat(use_stream=False):\n",
    "    messages = [{\n",
    "            \"role\": \"user\", \"content\": \"What's the Celsius temperature in San Francisco?\"\n",
    "        },]\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_current_weather\",\n",
    "                \"description\": \"Get the current weather\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                        },\n",
    "                        \"format\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                            \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"location\", \"format\"],\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"glm-4\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        stream=use_stream,\n",
    "        max_tokens=256,\n",
    "        temperature=0.9,\n",
    "        presence_penalty=1.2,\n",
    "        top_p=0.1,\n",
    "        tool_choice=\"auto\"\n",
    "    )\n",
    "    if response:\n",
    "        if use_stream:\n",
    "            for chunk in response:\n",
    "                print(chunk)\n",
    "        else:\n",
    "            print(response)\n",
    "    else:\n",
    "        print(\"Error:\", response.status_code)\n",
    "\n",
    "# 调用工具函数\n",
    "function_chat(use_stream=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b109b5ce-aee3-4a96-aa7a-11a5d54e58a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
