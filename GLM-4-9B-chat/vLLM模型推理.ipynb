{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a13f50d-b41d-4e01-822c-41069867778a",
   "metadata": {},
   "source": [
    "\n",
    "# vLLMè¿›è¡ŒæŽ¨ç†\n",
    "å®‰è£… vllm>=0.5.2 å’Œ flash-attn>=2.5.9 # using with flash-attention 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0412afc9-9a23-4a57-9115-ece8ec88822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vllm>=0.5.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a90d4060-b5b1-4ecd-90e6-e0977a4a0578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-31 21:27:36 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='/root/autodl-tmp/ZhipuAI/glm-4-9b-chat', speculative_config=None, tokenizer='/root/autodl-tmp/ZhipuAI/glm-4-9b-chat', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/root/autodl-tmp/ZhipuAI/glm-4-9b-chat, use_v2_block_manager=False, enable_prefix_caching=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-31 21:27:37 tokenizer.py:129] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "INFO 07-31 21:27:37 model_runner.py:680] Starting to load model /root/autodl-tmp/ZhipuAI/glm-4-9b-chat...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee448dde75b4327992877ff5ca122a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/10 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-31 21:27:40 model_runner.py:692] Loading model weights took 17.5635 GB\n",
      "INFO 07-31 21:27:44 gpu_executor.py:102] # GPU blocks: 2230, # CPU blocks: 6553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.75it/s, est. speed input: 22.08 toks/s, output: 44.15 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ä½ å¥½ðŸ‘‹ï¼å¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿Žé—®æˆ‘ä»»ä½•é—®é¢˜ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# GLM-4-9B-Chat-1M\n",
    "# max_model_len, tp_size = 1048576, 4\n",
    "\n",
    "# GLM-4-9B-Chat\n",
    "# å¦‚æžœé‡è§ OOM çŽ°è±¡ï¼Œå»ºè®®å‡å°‘max_model_lenï¼Œæˆ–è€…å¢žåŠ tp_size\n",
    "max_model_len, tp_size = 32768, 1\n",
    "#model_name = \"THUDM/glm-4-9b-chat\"\n",
    "model_name = '/root/autodl-tmp/ZhipuAI/glm-4-9b-chat'\n",
    "\n",
    "prompt = [{\"role\": \"user\", \"content\": \"ä½ å¥½\"}]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "llm = LLM(\n",
    "    model=model_name,\n",
    "    tensor_parallel_size=tp_size,\n",
    "    max_model_len=max_model_len,\n",
    "    trust_remote_code=True,\n",
    "    enforce_eager=True,\n",
    "    gpu_memory_utilization=1\n",
    "    # GLM-4-9B-Chat-1M å¦‚æžœé‡è§ OOM çŽ°è±¡ï¼Œå»ºè®®å¼€å¯ä¸‹è¿°å‚æ•°\n",
    "    # enable_chunked_prefill=True,\n",
    "    # max_num_batched_tokens=8192\n",
    ")\n",
    "stop_token_ids = [151329, 151336, 151338]\n",
    "sampling_params = SamplingParams(temperature=0.95, max_tokens=1024, stop_token_ids=stop_token_ids)\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n",
    "outputs = llm.generate(prompts=inputs, sampling_params=sampling_params)\n",
    "\n",
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca4e1a3-3a85-4f9b-b981-9f00adc2d474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
